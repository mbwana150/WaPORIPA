{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDDyUu-vXc-Y"
      },
      "source": [
        "#Module 1b: Aggregating pyWaPOR outputs into dekadal/ monthly and seasonal maps\n",
        "\n",
        "In this Notebook we use a script that was developed for the WaPOR project to calculate temporal aggregation of pyWaPOR output (AETI, T and NPP), which are needed to run the irrigation performance assessment module.\n",
        "This script exports the WaPOR netCDF file into dekadal/ monthly and seasonal TIFF maps.\n",
        "\n",
        "This script follows the following steps:\n",
        "\n",
        "*  Step 0: Import modules/libraries\n",
        "*  Step 1: Define functions for aggregating pyWaPOR outputs\n",
        "*  Step 2: Read pywapor output and reproject the xarray\n",
        "*  Step 3: Aggregate to the required timestep\n",
        "*  Step 4: Write the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnBJNMZ2M6Vr"
      },
      "source": [
        "## Step 0. Import modules/libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bdXzjxqXrZx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade xarray --quiet\n",
        "!pip install --upgrade rioxarray --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOJ77jsKXWuM"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import rioxarray as rio\n",
        "from rasterio.warp import reproject, Resampling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQs6DuCIO6bQ"
      },
      "source": [
        "## Step 1: Define function to aggregate pyWaPOR output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9En9y1jrXWuN"
      },
      "outputs": [],
      "source": [
        "# functions to calculate dekadal, monthly and seasonal sums from the ET_look output netcdf file\n",
        "def custom_sum(da):\n",
        "    da = da.where(da>=0, np.nan)\n",
        "    time_len = len(da.time)\n",
        "    da_mean = da.mean(dim = 'time', skipna=True)\n",
        "    return da_mean*time_len\n",
        "\n",
        "def resotor_encoding(ds, encoding, var_attrs, temp_res):\n",
        "    # Restore encoding information\n",
        "    for var in ds.data_vars:\n",
        "        if var in encoding:  # Ensure the variable exists in the original encoding\n",
        "            ds[var].encoding = encoding[var]\n",
        "\n",
        "    for var in ds.data_vars:\n",
        "        if var in var_attrs:  # Ensure the variable exists in the original encoding\n",
        "            attrs = ds[var].attrs\n",
        "            attrs_to_delete = [j for j in attrs if 'NETCDF_' in j or 'scale_factor' in j]\n",
        "            attrs = {key: attrs[key] for key in attrs if key not in attrs_to_delete}\n",
        "            lname = attrs['long_name']\n",
        "            lname = lname.replace(\"Daily\", temp_res).replace(\"mm\",'')\n",
        "            attrs.update({'long_name': lname, \n",
        "                          'source_data': 'Aggregated from ET_Look model output',\n",
        "                          'units' : f\"mm/{temp_res[:-2]}\",\n",
        "                          'temporal_resolution' : temp_res,})\n",
        "            ds[var].attrs = attrs\n",
        "    return ds\n",
        "\n",
        "# decadal sum\n",
        "def dekadal_sum(ds):\n",
        "    # Store encoding information\n",
        "    encoding = {var: ds[var].encoding for var in ds.data_vars}\n",
        "    attrs = {var: ds[var].attrs for var in ds.data_vars}\n",
        "    # aggegate to dekadal values (10,10, and 10/11/9/8 based on the eyar and the month)\n",
        "    d = ds.time.dt.day - np.clip((ds.time.dt.day-1) // 10, 0, 2)*10 - 1\n",
        "    date = ds.time.values - np.array(d, dtype=\"timedelta64[D]\")\n",
        "    ds['time'] = date\n",
        "    ds_dk = ds.groupby(ds.time).map(custom_sum)\n",
        "    # Restore encoding information\n",
        "    ds_dk = resotor_encoding(ds_dk, encoding, attrs, 'Dekadal') #resotor_encoding(ds_dk, encoding, attrs, 'dekadal')\n",
        "    return ds_dk\n",
        "\n",
        "# Monthly sum\n",
        "def monthly_sum(ds):\n",
        "    # Store encoding information\n",
        "    encoding = {var: ds[var].encoding for var in ds.data_vars}\n",
        "    attrs = {var: ds[var].attrs for var in ds.data_vars}\n",
        "    # aggregate to monthly\n",
        "    ds_mn = ds.resample(time=\"1ME\").map(custom_sum)\n",
        "    # Restore encoding information\n",
        "    ds_mn = resotor_encoding(ds_mn, encoding, attrs, 'Monthly')\n",
        "    return ds_mn\n",
        "# Check if the start and end time of the selected dataarray corresponds to sos and eos\n",
        "def select_season_da(da_var, season_start_date, season_end_date):\n",
        "\n",
        "    sos = datetime.datetime.fromisoformat(season_start_date) #start of season date, we use datetime.datetime to convert the year, month, day to a datetime object\n",
        "    eos = datetime.datetime.fromisoformat(season_end_date) #end of season date\n",
        "\n",
        "    da_st = datetime.datetime.fromisoformat(pd.to_datetime(da_var.time.data).strftime('%Y-%m-%d')[0])\n",
        "    da_et = datetime.datetime.fromisoformat(pd.to_datetime(da_var.time.data).strftime('%Y-%m-%d')[-1])\n",
        "    try:\n",
        "        if (sos >= da_st) or (eos <= da_et):\n",
        "            da = da_var.sel(time=slice(sos, eos))\n",
        "            return da\n",
        "        else:\n",
        "            print(\"The sos and/or eos out of the time range of the dataset.\")\n",
        "            da = da_var.sel(time=slice(sos, eos))\n",
        "            return da\n",
        "    except ValueError:\n",
        "        print(\"Erro in selecting data for the season.\")\n",
        "\n",
        "# Seasonal Resample\n",
        "def seasonal_sum(ds, sos, eos):\n",
        "    # Store encoding information\n",
        "    encoding = {var: ds[var].encoding for var in ds.data_vars}\n",
        "    attrs = {var: ds[var].attrs for var in ds.data_vars}\n",
        "\n",
        "    # seasonal sum\n",
        "    ds_sn = select_season_da(ds, sos, eos).map(custom_sum)\n",
        "    \n",
        "    # Restore encoding information\n",
        "    ds_sn = resotor_encoding(ds_sn, encoding, attrs, 'Seasonal')\n",
        "    for var in ds_sn.data_vars:\n",
        "        if var in encoding:  # Ensure the variable exists in the original encoding\n",
        "            encoding[var]['sos']= sos\n",
        "            encoding[var]['eos']= eos\n",
        "            ds_sn[var].encoding = encoding[var]\n",
        "    return ds_sn\n",
        "  \n",
        "def reproject(ds, to_crs):\n",
        "#   encoding = ds.econding #{var: ds[var].encoding for var in ds.data_vars}\n",
        "  try:\n",
        "      if 'EPSG' in str(to_crs):\n",
        "        #   print(to_crs)\n",
        "          dst = ds.rio.reproject(to_crs)\n",
        "          return dst\n",
        "      else: # assume it is a path to template file\n",
        "        if os.path.exists(to_crs):\n",
        "            # print(\"Use a template raster to repoject the dataset\")\n",
        "            temp_rst_file = to_crs\n",
        "            da_rst = rio.open_rasterio(temp_rst_file)\n",
        "            if da_rst.rio.crs != None:\n",
        "                dst= ds.rio.reproject_match(da_rst)\n",
        "                return dst\n",
        "            else:\n",
        "                print(f\"the template raster {temp_rst_file} does not have CRS information.\")\n",
        "  except ValueError:\n",
        "        print(\"Your input is not either a valid EPSG code or a teplate raster path.\")\n",
        "\n",
        "\n",
        "switcher = {\n",
        "        'et': 'AETI',\n",
        "        'e': 'E',\n",
        "        'int': 'I',\n",
        "        'npp': 'NPP',\n",
        "        't': 'T',\n",
        "        'se': 'RSM'\n",
        "    }\n",
        "def get_var_name(var_name):\n",
        "    func = switcher.get(var_name, \"nothing\")\n",
        "    # Execute the function\n",
        "    return func\n",
        "switcher2 = {\n",
        "        'dekadal': 'D',\n",
        "        'monthly': 'M',\n",
        "        'seasonal': 'S',\n",
        "    }\n",
        "def get_time_code(temporal_res):\n",
        "    func = switcher2.get(temporal_res, \"nothing\")\n",
        "    # Execute the function\n",
        "    return func\n",
        "\n",
        "# write to file\n",
        "def write_file(da, to_crs, fname, encoding, date, attrs, temporal_res) :\n",
        "    if(to_crs!=None):\n",
        "        # reproject the data. provide a crs in the form of f\"EPSG:{epsg code}\" or a path to template raster\n",
        "        da = reproject(da, to_crs)\n",
        "    # Modify the attributes\n",
        "    attrs.update({'date': date})\n",
        "    da.attrs  = attrs\n",
        "    da = da.round(2)\n",
        "    da.encoding = encoding   #['scale_factor'] = 1.0\n",
        "    da.rio.to_raster(f\"{fname}.tif\", driver=\"GTiff\", compress=\"LZW\")\n",
        "    da.close()\n",
        "\n",
        "# netCDF to geotiff\n",
        "def write2gtiff(ds, temporal_res, dir_out, to_crs = None):\n",
        "  \n",
        "  if 'time' in ds.dims:\n",
        "      date_str = pd.to_datetime(ds.time.data).strftime('%Y-%m-%d')\n",
        "\n",
        "  for var in ds.data_vars:\n",
        "    var_name = get_var_name(var.split('_')[0])\n",
        "    time_code = get_time_code(temporal_res)\n",
        "    var_name = f\"{var_name}_{time_code}\"\n",
        "    fd = os.path.join(dir_out, temporal_res, f\"pywapor_{var_name}\")\n",
        "    encoding  = ds[var].encoding\n",
        "    encoding['dtype'] = 'float32'\n",
        "    encoding['scale_factor'] = 1.0\n",
        "    encoding['_FillValue'] = encoding['_FillValue'].astype('float32')\n",
        "    # print(encoding)\n",
        "    attrs = ds[var].attrs\n",
        "    if(temporal_res.lower() == 'seasonal'):\n",
        "        sos = encoding['sos']\n",
        "        eos = encoding['eos']\n",
        "    # Create folder per variable.\n",
        "    if not os.path.isdir(fd):\n",
        "        os.makedirs(fd)\n",
        "\n",
        "    if(temporal_res.lower() != 'seasonal'):\n",
        "        for i in range(len(ds.time)):\n",
        "            date = date_str[i]\n",
        "            fname = os.path.join(fd, f\"pywapor_{var_name}_{date}\")\n",
        "            da = ds[var][i]\n",
        "            da = da.drop_vars('time')  # get the data for one time step\n",
        "            write_file(da, to_crs, fname, encoding, date, attrs, temporal_res)    \n",
        "    else:\n",
        "        date = f\"{sos}_{eos}\"\n",
        "        fname = os.path.join(fd, f\"pywapor_{var_name}_{date}\")\n",
        "        da = ds[var]\n",
        "        write_file(da, to_crs, fname, encoding, date, attrs, temporal_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpPAxwywXWuP"
      },
      "source": [
        "## Step 2: Read pywapor output \n",
        "\n",
        "Set the path to the ET Look output file and select the dataset for the time of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74LRxs82XWuR"
      },
      "outputs": [],
      "source": [
        "# path to the et_look_out/nc file\n",
        "path_et_look_out = r'/content/et_look_out.nc'\n",
        "ds = xr.open_dataset(path_et_look_out, decode_coords=\"all\")\n",
        "ds = ds.rename({'time_bins': 'time'})\n",
        "\n",
        "# select the period for time of interest wich may be a season\n",
        "season = [\"2022-11-01\", \"2023-01-31\"]\n",
        "ds = ds.sel(time=slice(season[0], season[1]))\n",
        "# ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQIVRmFJXWuT"
      },
      "source": [
        "## Step 3: Aggregate to the required timestep (dekadal, monthly or seasonal) and write the result to individual TIFF files per time step\n",
        "\n",
        "The ET_look output is in EPSG:4326, if you would like to reproject the dataset to other projections such UTM zone, provide the required epsg code or a path to raster template file. The defualt is an estimated utm crs from the dataset. if you want to change provide the crs in the following style: to_crs = f\"EPSG:{epsg code (number)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir_out = r'/content/output' # a folder in your gdrive to save the geotif files\n",
        "if not os.path.exists(dir_out):\n",
        "    os.makedirs(dir_out)\n",
        "    \n",
        "# estinated utm crs from the dataset\n",
        "to_crs = ds.rio.estimate_utm_crs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aggregate to dekadal timestep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# aggregate to dekadal timestep\n",
        "ds_dk = dekadal_sum(ds) # dekadal\n",
        "temporal_res = 'dekadal'\n",
        "write2gtiff(ds_dk, temporal_res, dir_out, to_crs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aggregate to monthly timestep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# aggregate to monthly timestep\n",
        "ds_mn = monthly_sum(ds) # monthly\n",
        "temporal_res = 'monthly'\n",
        "write2gtiff(ds_mn, temporal_res, dir_out, to_crs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aggregate to a season"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# aggregate to a season\n",
        "season = ['2022-10-01', '2023-04-30'] # start and end of season (dates in iso format)\n",
        "ds_sn = seasonal_sum(ds, season[0], season[1])\n",
        "temporal_res = 'seasonal'\n",
        "write2gtiff(ds_sn, temporal_res, dir_out, to_crs )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBimzqekXWuU"
      },
      "outputs": [],
      "source": [
        "# # test opening one of the geotiff files\n",
        "# rst_path = r'/content/output/dekadal/e/e_2022-10-01.tif'\n",
        "# da = rio.open_rasterio(rst_path)\n",
        "# da.plot()\n",
        "# da.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zip and download the data folder to your local drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a2KLUFxiZNr"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/pywapor_out.zip /content/output\n",
        "from google.colab import files\n",
        "files.download(r'/content/pywapor_out.zip')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pywapor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
